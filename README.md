# Text-prediction-using-RNNs
An educational project exploring text prediction using Recurrent Neural Networks (RNNs) from scratch. Built to understand sequence modeling, language preprocessing, and deep learning basics.
# Text Prediction with RNNs (Learning Project)

This repo is a learning project on **text prediction using Recurrent Neural Networks (RNNs)**.  
It explores the full path from **text preprocessing** to **training an RNN from scratch**.

## Contents
- `preprocessing_timemachine.ipynb` → Tokenization, vocabulary building, and pattern analysis on the *Time Machine* dataset.
- `rnn_from_scratch.ipynb` → Implementation of a simple RNN model from scratch for text prediction.

## Goals
- Understand basic NLP preprocessing (tokenization, vocabularization, n-gram counting).
- Learn how RNNs handle sequential data.
- Build and train an RNN without much help from high-level frameworks.

## Dataset
The project uses the public-domain text *The Time Machine* by H. G. Wells.

## Notes
This is not meant to be a production-ready model, but rather a **step-by-step learning exercise** in sequence modeling and deep learning.
